{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5451b3a-cbf0-4037-9f9c-825a17ae54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "INPUT_CSV = \"merged original mentions from altmetric.csv\"\n",
    "OUT_CLEAN_CSV = \"mentions_original_cleaned.csv\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    INPUT_CSV,\n",
    "    dtype=str,\n",
    "    keep_default_na=False,\n",
    "    na_values=[]\n",
    ")\n",
    "\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "keep_types = {\"news story\", \"news\", \"blog post\", \"blog\"}\n",
    "if \"Mention Type\" in df.columns:\n",
    "    mask = df[\"Mention Type\"].str.lower().isin(keep_types)\n",
    "    df = df[mask].copy()\n",
    "\n",
    "if \"Mention URL\" in df.columns:\n",
    "    URL_COL = \"Mention URL\"\n",
    "else:\n",
    "    url_like = [c for c in df.columns if \"url\" in c.lower()]\n",
    "    URL_COL = url_like[0] if url_like else None\n",
    "\n",
    "if URL_COL is None:\n",
    "    df[\"Mention URL\"] = \"\"\n",
    "else:\n",
    "    df[\"Mention URL\"] = df[URL_COL].astype(str).str.strip()\n",
    "\n",
    "df.loc[df[\"Mention URL\"].str.lower().eq(\"nan\"), \"Mention URL\"] = \"\"\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    s = u.strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    try:\n",
    "        uo = urlparse(s)\n",
    "        if uo.scheme.lower() not in {\"http\", \"https\"}:\n",
    "            return \"\"\n",
    "        q = parse_qs(uo.query, keep_blank_values=True)\n",
    "        for k in list(q.keys()):\n",
    "            lk = k.lower()\n",
    "            if lk.startswith(\"utm_\") or lk in {\"fbclid\", \"gclid\"}:\n",
    "                q.pop(k, None)\n",
    "        new_q = urlencode(q, doseq=True)\n",
    "        path = uo.path[:-1] if uo.path.endswith(\"/\") else uo.path\n",
    "        return urlunparse((uo.scheme.lower(), uo.netloc.lower(), path, uo.params, new_q, \"\"))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "df[\"norm_url\"] = df[\"Mention URL\"].map(normalize_url)\n",
    "\n",
    "dropped_empty = df[df[\"norm_url\"] == \"\"].copy()\n",
    "kept = df[df[\"norm_url\"] != \"\"].copy()\n",
    "\n",
    "dedup_keys = [\"norm_url\"]\n",
    "for c in [\"Details Page URL\", \"DOI\", \"PubMed ID\", \"ArXiv ID\", \"Patent Number\", \"Mention Type\"]:\n",
    "    if c in kept.columns:\n",
    "        dedup_keys.append(c)\n",
    "\n",
    "sort_cols = [\"Mention Date\"] if \"Mention Date\" in kept.columns else dedup_keys\n",
    "mentions_clean = (kept.sort_values(sort_cols)\n",
    "                       .drop_duplicates(subset=dedup_keys)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "mentions_clean.to_csv(OUT_CLEAN_CSV, index=False)\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"Total rows in CSV:           {len(df)}\")\n",
    "print(f\"Rows dropped (empty URL):    {len(dropped_empty)}\")\n",
    "print(f\"Rows kept (valid URL):       {len(kept)}\")\n",
    "print(f\"Cleaned mentions (pairwise): {len(mentions_clean)} -> {OUT_CLEAN_CSV}\")\n",
    "print(\"Dedup keys:\", dedup_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48762c66-ae15-450f-bed4-c1b316aef83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode\n",
    "\n",
    "INPUT_CSV = \"mentions_retraction_cleaned.csv\"\n",
    "OUT_CLEAN_CSV = \"merged retraction mentions from altmetric.csv\"\n",
    "\n",
    "df = pd.read_csv(\n",
    "    INPUT_CSV,\n",
    "    dtype=str,\n",
    "    keep_default_na=False,\n",
    "    na_values=[]\n",
    ")\n",
    "\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "keep_types = {\"news story\", \"news\", \"blog post\", \"blog\"}\n",
    "if \"Mention Type\" in df.columns:\n",
    "    mask = df[\"Mention Type\"].str.lower().isin(keep_types)\n",
    "    df = df[mask].copy()\n",
    "\n",
    "if \"Mention URL\" in df.columns:\n",
    "    URL_COL = \"Mention URL\"\n",
    "else:\n",
    "    url_like = [c for c in df.columns if \"url\" in c.lower()]\n",
    "    URL_COL = url_like[0] if url_like else None\n",
    "\n",
    "if URL_COL is None:\n",
    "    df[\"Mention URL\"] = \"\"\n",
    "else:\n",
    "    df[\"Mention URL\"] = df[URL_COL].astype(str).str.strip()\n",
    "\n",
    "df.loc[df[\"Mention URL\"].str.lower().eq(\"nan\"), \"Mention URL\"] = \"\"\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    if not isinstance(u, str):\n",
    "        return \"\"\n",
    "    s = u.strip()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    try:\n",
    "        uo = urlparse(s)\n",
    "        if uo.scheme.lower() not in {\"http\", \"https\"}:\n",
    "            return \"\"\n",
    "        q = parse_qs(uo.query, keep_blank_values=True)\n",
    "        for k in list(q.keys()):\n",
    "            lk = k.lower()\n",
    "            if lk.startswith(\"utm_\") or lk in {\"fbclid\", \"gclid\"}:\n",
    "                q.pop(k, None)\n",
    "        new_q = urlencode(q, doseq=True)\n",
    "        path = uo.path[:-1] if uo.path.endswith(\"/\") else uo.path\n",
    "        return urlunparse((uo.scheme.lower(), uo.netloc.lower(), path, uo.params, new_q, \"\"))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "df[\"norm_url\"] = df[\"Mention URL\"].map(normalize_url)\n",
    "\n",
    "dropped_empty = df[df[\"norm_url\"] == \"\"].copy()\n",
    "kept = df[df[\"norm_url\"] != \"\"].copy()\n",
    "\n",
    "dedup_keys = [\"norm_url\"]\n",
    "for c in [\"Details Page URL\", \"DOI\", \"PubMed ID\", \"ArXiv ID\", \"Patent Number\", \"Mention Type\"]:\n",
    "    if c in kept.columns:\n",
    "        dedup_keys.append(c)\n",
    "\n",
    "sort_cols = [\"Mention Date\"] if \"Mention Date\" in kept.columns else dedup_keys\n",
    "mentions_clean = (kept.sort_values(sort_cols)\n",
    "                       .drop_duplicates(subset=dedup_keys)\n",
    "                       .reset_index(drop=True))\n",
    "\n",
    "mentions_clean.to_csv(OUT_CLEAN_CSV, index=False)\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"Total rows in CSV:           {len(df)}\")\n",
    "print(f\"Rows dropped (empty URL):    {len(dropped_empty)}\")\n",
    "print(f\"Rows kept (valid URL):       {len(kept)}\")\n",
    "print(f\"Cleaned mentions (pairwise): {len(mentions_clean)} -> {OUT_CLEAN_CSV}\")\n",
    "print(\"Dedup keys:\", dedup_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
