{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fc2ac-7c97-4173-96e5-f15347279bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json, random, time, asyncio, sys\n",
    "import pandas as pd\n",
    "import tldextract\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, urlunparse, parse_qs, urlencode, urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_CSV   = \"mentions_original_part1.csv\" \n",
    "OUTPUT_CSV  = \"mentions_original_part1_fill.csv\" \n",
    "\n",
    "MAX_CONCURRENCY              = 50\n",
    "MAX_CONCURRENCY_PER_HOST     = 6\n",
    "REQUEST_TIMEOUT              = 25\n",
    "CONNECT_TIMEOUT              = 8\n",
    "RETRY_MAX                    = 3\n",
    "BACKOFF_BASE                 = 1.5\n",
    "SLEEP_BETWEEN_BATCHES        = (0.0, 0.0)\n",
    "\n",
    "USER_AGENT = \"USyd-Altmetric-4.4/async/1.0\"\n",
    "SUSPECT_HOSTS = {\n",
    "    \"forbetterscience.com\",\n",
    "    \"retractionwatch.com\",\n",
    "    \"whyevolutionistrue.com\",\n",
    "    \"rawnews.com\",\n",
    "    \"ct.moreover.com\",\n",
    "}\n",
    "ALLOW_CROSS_DOMAIN_HOSTS = {\n",
    "    \"ct.moreover.com\", \"feedproxy.google.com\", \"feedburner.com\",\n",
    "}\n",
    "\n",
    "EXTRA_HEADERS = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "FORCE_SAME_DOMAIN_FINAL_URL = True\n",
    "\n",
    "MIN_TEXT_LEN_FOR_OK   = 20\n",
    "HARD_MAX_TEXT_CHARS   = 50000\n",
    "\n",
    "REFETCH_BAD = True\n",
    "RETRY_STATUS_SET = {403, 429, 410, 500, 502, 503, 504}\n",
    "\n",
    "MIN_DIMENSION_PX   = 120\n",
    "FILENAME_NEG_PAT   = re.compile(r\"(logo|icon|sprite|avatar|ads?|banner|pixel|track|spacer|blank)\", re.I)\n",
    "CLASS_NEG_PAT      = re.compile(r\"(logo|icon|avatar|ads?|banner|breadcrumb|nav|footer|header)\", re.I)\n",
    "WP_POS_PAT         = re.compile(r\"(wp-post-image|entry-content|size-\\w+)\", re.I)\n",
    "AROUND_TEXT_CHARS  = 40\n",
    "KEEP_OGIMAGE_IF_SAME_DOMAIN = True\n",
    "\n",
    "if 'ipykernel' in sys.modules:\n",
    "    try:\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def normalize_url(u: str) -> str:\n",
    "    if not isinstance(u, str): return \"\"\n",
    "    s = u.strip()\n",
    "    if not s: return \"\"\n",
    "    try:\n",
    "        uo = urlparse(s)\n",
    "        if uo.scheme.lower() not in {\"http\",\"https\"}: return \"\"\n",
    "        q = parse_qs(uo.query, keep_blank_values=True)\n",
    "        for k in list(q.keys()):\n",
    "            lk = k.lower()\n",
    "            if lk.startswith(\"utm_\") or lk in {\"fbclid\",\"gclid\"}:\n",
    "                q.pop(k, None)\n",
    "        new_q = urlencode(q, doseq=True)\n",
    "        path = uo.path[:-1] if uo.path.endswith(\"/\") else uo.path\n",
    "        return urlunparse((uo.scheme.lower(), uo.netloc.lower(), path, uo.params, new_q, \"\"))\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def host_of(u):\n",
    "    try:\n",
    "        return urlparse(u).netloc.split(\":\")[0].lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def same_domain(u1, u2):\n",
    "    try:\n",
    "        a = urlparse(u1).netloc.split(\":\")[0].lower()\n",
    "        b = urlparse(u2).netloc.split(\":\")[0].lower()\n",
    "        return a == b or a.endswith(b) or b.endswith(a)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def pick_article_container(soup: BeautifulSoup):\n",
    "    node = soup.find(\"article\")\n",
    "    if node: return node\n",
    "    node = soup.find(\"main\")\n",
    "    if node: return node\n",
    "    for kw in [\"article\",\"post\",\"story\",\"content\",\"entry\",\"article-body\",\"post-content\",\"rich-text\"]:\n",
    "        cand = soup.find(\"div\", class_=re.compile(kw, re.I)) or soup.find(\"section\", class_=re.compile(kw, re.I))\n",
    "        if cand: return cand\n",
    "    return soup.body or soup\n",
    "\n",
    "def nearby_text_len(el):\n",
    "    total = 0\n",
    "    if el.parent:\n",
    "        total += len(el.parent.get_text(separator=\" \", strip=True) or \"\")\n",
    "    for sib in list(el.previous_siblings)[:2] + list(el.next_siblings)[:2]:\n",
    "        try:\n",
    "            if hasattr(sib, \"get_text\"):\n",
    "                total += len(sib.get_text(separator=\" \", strip=True) or \"\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return total\n",
    "\n",
    "def parse_max_srcset_width(srcset_val: str):\n",
    "    try:\n",
    "        widths = []\n",
    "        for part in srcset_val.split(\",\"):\n",
    "            part = part.strip()\n",
    "            m = re.search(r\"\\s(\\d+)w\", part)\n",
    "            if m:\n",
    "                widths.append(int(m.group(1)))\n",
    "        return max(widths) if widths else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_relevant_images(html: str, base_url: str):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        return []\n",
    "    article = pick_article_container(soup)\n",
    "    out, seen = [], set()\n",
    "    metas = []\n",
    "    for sel in [\"meta[property='og:image']\", \"meta[name='twitter:image']\"]:\n",
    "        for m in soup.select(sel):\n",
    "            u = (m.get(\"content\") or \"\").strip()\n",
    "            if not u: continue\n",
    "            u = urljoin(base_url, u)\n",
    "            metas.append(u)\n",
    "    def _same_domain(u):\n",
    "        try:\n",
    "            a = urlparse(base_url).netloc.split(\":\")[0].lower()\n",
    "            b = urlparse(u).netloc.split(\":\")[0].lower()\n",
    "            return a == b or a.endswith(b) or b.endswith(a)\n",
    "        except Exception:\n",
    "            return False\n",
    "    for u in metas:\n",
    "        fn = u.split(\"/\")[-1].lower()\n",
    "        if FILENAME_NEG_PAT.search(fn):\n",
    "            continue\n",
    "        if KEEP_OGIMAGE_IF_SAME_DOMAIN and not _same_domain(u):\n",
    "            continue\n",
    "        if u not in seen:\n",
    "            seen.add(u); out.append(u)\n",
    "    for img in article.find_all(\"img\"):\n",
    "        src = (img.get(\"src\") or img.get(\"data-src\") or img.get(\"data-original\") or \"\").strip()\n",
    "        if not src: continue\n",
    "        u = urljoin(base_url, src)\n",
    "        if not u.startswith(\"http\"): continue\n",
    "        fn = u.split(\"/\")[-1].lower()\n",
    "        if FILENAME_NEG_PAT.search(fn): continue\n",
    "        cls = \" \".join(img.get(\"class\", [])).lower()\n",
    "        if CLASS_NEG_PAT.search(cls) and not WP_POS_PAT.search(cls):\n",
    "            continue\n",
    "        def parse_dim(v):\n",
    "            try:\n",
    "                v = str(v).lower().strip().replace(\"px\",\"\")\n",
    "                return int(re.sub(r\"[^\\d]\", \"\", v)) if re.search(r\"\\d\", v) else None\n",
    "            except Exception:\n",
    "                return None\n",
    "        w = parse_dim(img.get(\"width\"))\n",
    "        h = parse_dim(img.get(\"height\"))\n",
    "        if not w and img.get(\"srcset\"):\n",
    "            w = parse_max_srcset_width(img.get(\"srcset\"))\n",
    "        if (w and w < MIN_DIMENSION_PX) or (h and h < MIN_DIMENSION_PX):\n",
    "            alt = (img.get(\"alt\") or \"\").strip()\n",
    "            if len(alt) < 12 and not img.find_parent(\"a\"):\n",
    "                continue\n",
    "        has_caption = bool(img.find_parent(\"figure\") and img.find_parent(\"figure\").find(\"figcaption\"))\n",
    "        near_text = nearby_text_len(img) >= AROUND_TEXT_CHARS\n",
    "        alt_long  = len((img.get(\"alt\") or \"\").strip()) >= 12\n",
    "        anchor_wrapped = bool(img.find_parent(\"a\"))\n",
    "        if not (has_caption or near_text or alt_long or anchor_wrapped or WP_POS_PAT.search(cls)):\n",
    "            continue\n",
    "        if u not in seen:\n",
    "            seen.add(u); out.append(u)\n",
    "    return out\n",
    "\n",
    "JS_REDIRECT_PAT = re.compile(r\"\"\"(?:window|document)\\.location(?:\\.href)?\\s*=\\s*['\"]([^'\"]+)['\"]\"\"\", re.I)\n",
    "META_REFRESH_PAT = re.compile(r\"^\\s*\\d+\\s*;\\s*url\\s*=\\s*(.+)\\s*$\", re.I)\n",
    "\n",
    "def discover_target_url(html: str, base_url: str) -> str:\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    canon = soup.find(\"link\", rel=lambda v: v and \"canonical\" in v.lower())\n",
    "    if canon and canon.get(\"href\"):\n",
    "        return urljoin(base_url, canon.get(\"href\").strip())\n",
    "    ogu = soup.find(\"meta\", attrs={\"property\":\"og:url\"})\n",
    "    if ogu and ogu.get(\"content\"):\n",
    "        return urljoin(base_url, ogu.get(\"content\").strip())\n",
    "    mref = soup.find(\"meta\", attrs={\"http-equiv\": lambda v: v and v.lower()==\"refresh\"})\n",
    "    if mref and mref.get(\"content\"):\n",
    "        c = mref.get(\"content\").strip()\n",
    "        m = META_REFRESH_PAT.match(c)\n",
    "        if m:\n",
    "            return urljoin(base_url, m.group(1).strip())\n",
    "    for s in soup.find_all(\"script\"):\n",
    "        txt = s.string or \"\"\n",
    "        m = JS_REDIRECT_PAT.search(txt)\n",
    "        if m:\n",
    "            return urljoin(base_url, m.group(1).strip())\n",
    "    return \"\"\n",
    "\n",
    "import aiohttp\n",
    "from aiohttp import ClientTimeout\n",
    "\n",
    "global_semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "host_semaphores = {}\n",
    "\n",
    "def get_host_sema(url):\n",
    "    host = host_of(url)\n",
    "    limit = 2 if host in SUSPECT_HOSTS else MAX_CONCURRENCY_PER_HOST\n",
    "    if host not in host_semaphores:\n",
    "        host_semaphores[host] = asyncio.Semaphore(limit)\n",
    "    return host_semaphores[host]\n",
    "\n",
    "def build_timeout():\n",
    "    return ClientTimeout(total=None, connect=CONNECT_TIMEOUT, sock_connect=CONNECT_TIMEOUT, sock_read=REQUEST_TIMEOUT)\n",
    "\n",
    "async def fetch_one(session, url):\n",
    "    err = \"\"; final_url = url; status = None; html = None\n",
    "    host = host_of(url)\n",
    "    for attempt in range(1, RETRY_MAX+1):\n",
    "        try:\n",
    "            async with global_semaphore, get_host_sema(url):\n",
    "                hdrs = {\"User-Agent\": USER_AGENT, **EXTRA_HEADERS}\n",
    "                if attempt == RETRY_MAX and host in SUSPECT_HOSTS:\n",
    "                    hdrs[\"Referer\"] = f\"https://{host}/\"\n",
    "                async with session.get(url, allow_redirects=True, timeout=build_timeout(), headers=hdrs) as resp:\n",
    "                    status = resp.status\n",
    "                    final_url = str(resp.url)\n",
    "                    ctype = resp.headers.get(\"Content-Type\",\"\")\n",
    "                    if status == 200 and \"text/html\" in ctype:\n",
    "                        html = await resp.text(errors=\"ignore\")\n",
    "                        return status, html, final_url, \"\"\n",
    "                    if status in RETRY_STATUS_SET and attempt < RETRY_MAX:\n",
    "                        await asyncio.sleep((BACKOFF_BASE ** attempt) + random.random())\n",
    "                        continue\n",
    "                    return status, None, final_url, f\"http_{status}\"\n",
    "        except Exception as e:\n",
    "            err = str(e)\n",
    "        await asyncio.sleep((BACKOFF_BASE ** attempt) * 0.5 + random.random())\n",
    "    return status, html, final_url, (err or f\"http_{status}\")\n",
    "\n",
    "async def fetch_all(urls):\n",
    "    results = {}\n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENCY, ssl=False)\n",
    "    async with aiohttp.ClientSession(connector=connector, cookie_jar=aiohttp.DummyCookieJar()) as session:\n",
    "        async def fetch_with_url(u):\n",
    "            status, html, final_url, err = await fetch_one(session, u)\n",
    "            return u, status, html, final_url, err\n",
    "        tasks = [asyncio.create_task(fetch_with_url(u)) for u in urls]\n",
    "        for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Fetching\"):\n",
    "            u, status, html, final_url, err = await fut\n",
    "            results[u] = (status, html, final_url, err)\n",
    "    return results\n",
    "\n",
    "from readability import Document\n",
    "\n",
    "def extract_text_strong(html: str, base_url: str):\n",
    "    text0 = trafilatura.extract(html, include_comments=False, include_tables=False, no_fallback=True) or \"\"\n",
    "    method = \"trafilatura\"\n",
    "    if len(text0) < MIN_TEXT_LEN_FOR_OK:\n",
    "        try:\n",
    "            doc = Document(html)\n",
    "            content_html = doc.summary(html_partial=True)\n",
    "            if content_html:\n",
    "                soup = BeautifulSoup(content_html, \"lxml\")\n",
    "                t2 = soup.get_text(separator=\"\\n\", strip=True)\n",
    "                if len(t2) > len(text0):\n",
    "                    text0 = t2\n",
    "                    method = \"readability\"\n",
    "        except Exception:\n",
    "            pass\n",
    "    full_len = len(text0)\n",
    "    truncated = 0\n",
    "    text = text0\n",
    "    if full_len > HARD_MAX_TEXT_CHARS:\n",
    "        text = text0[:HARD_MAX_TEXT_CHARS]\n",
    "        truncated = 1\n",
    "    return text, len(text), method, truncated, full_len\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV, dtype=str, keep_default_na=False, na_values=[])\n",
    "if \"Mention URL\" not in df.columns:\n",
    "    raise SystemExit(\"The input is missing the 'Mention URL' column.\")\n",
    "\n",
    "df[\"Mention URL\"] = df[\"Mention URL\"].astype(str).str.strip()\n",
    "df[\"norm_url\"] = df[\"Mention URL\"].map(normalize_url)\n",
    "valid = df[df[\"norm_url\"] != \"\"].copy()\n",
    "\n",
    "unique_urls = sorted(valid[\"norm_url\"].unique())\n",
    "print(f\"Unique URLs to fetch: {len(unique_urls)}\")\n",
    "\n",
    "html_map = await fetch_all(unique_urls)\n",
    "\n",
    "cache = {}\n",
    "bad_urls = []\n",
    "\n",
    "for u in tqdm(unique_urls, desc=\"Parsing\"):\n",
    "    status, html, final_url, err = html_map.get(u, (None, None, u, \"no_result\"))\n",
    "    if status == 200 and html and FORCE_SAME_DOMAIN_FINAL_URL:\n",
    "        if host_of(u) not in ALLOW_CROSS_DOMAIN_HOSTS and not same_domain(u, final_url):\n",
    "            final_url = u\n",
    "    if status != 200 or not html:\n",
    "        cache[u] = dict(\n",
    "            final_url=final_url, http_status=status, error=err,\n",
    "            domain=\"\", page_title=\"\", article_title=\"\",\n",
    "            text=\"\", text_len=0, text_len_full=0,\n",
    "            extraction_method=\"none\", text_truncated=0,\n",
    "            images_json=\"[]\", images_count=0\n",
    "        )\n",
    "        if status in RETRY_STATUS_SET or (status == 200 and not html):\n",
    "            bad_urls.append(u)\n",
    "        continue\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "    except Exception:\n",
    "        soup = None\n",
    "    page_title, article_title = \"\", \"\"\n",
    "    if soup:\n",
    "        if soup.title and soup.title.string:\n",
    "            page_title = soup.title.string.strip()\n",
    "        else:\n",
    "            ogt = soup.select_one(\"meta[property='og:title']\")\n",
    "            if ogt and ogt.get(\"content\"):\n",
    "                page_title = ogt.get(\"content\").strip()\n",
    "        h1 = soup.find(\"h1\")\n",
    "        if h1 and h1.get_text(strip=True):\n",
    "            article_title = h1.get_text(strip=True)\n",
    "        elif not article_title:\n",
    "            ogt = soup.select_one(\"meta[property='og:title']\")\n",
    "            if ogt and ogt.get(\"content\"):\n",
    "                article_title = ogt.get(\"content\").strip()\n",
    "    text, text_len, extract_method, truncated, text_len_full = extract_text_strong(html, final_url)\n",
    "    images = extract_relevant_images(html, final_url)\n",
    "    if text_len < MIN_TEXT_LEN_FOR_OK:\n",
    "        target = discover_target_url(html, final_url)\n",
    "        if target and normalize_url(target) != normalize_url(final_url):\n",
    "            html_t = trafilatura.fetch_url(target)\n",
    "            if html_t:\n",
    "                t2, tl2, m2, tr2, fl2 = extract_text_strong(html_t, target)\n",
    "                if tl2 > text_len:\n",
    "                    text, text_len, extract_method, truncated, text_len_full = t2, tl2, m2, tr2, fl2\n",
    "                    images2 = extract_relevant_images(html_t, target)\n",
    "                    if len(images2) >= len(images):\n",
    "                        images = images2\n",
    "                    final_url = target\n",
    "                    try:\n",
    "                        soup2 = BeautifulSoup(html_t, \"lxml\")\n",
    "                        if soup2.title and soup2.title.string:\n",
    "                            page_title = soup2.title.string.strip() or page_title\n",
    "                        h1 = soup2.find(\"h1\")\n",
    "                        if h1 and h1.get_text(strip=True):\n",
    "                            article_title = h1.get_text(strip=True)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    ext = tldextract.extract(final_url)\n",
    "    domain = \".\".join([p for p in [ext.domain, ext.suffix] if p])\n",
    "    cache[u] = dict(\n",
    "        final_url=final_url, http_status=status, error=\"\",\n",
    "        domain=domain, page_title=page_title, article_title=article_title,\n",
    "        text=text, text_len=text_len, text_len_full=text_len_full,\n",
    "        extraction_method=extract_method, text_truncated=truncated,\n",
    "        images_json=json.dumps(images, ensure_ascii=False), images_count=len(images)\n",
    "    )\n",
    "    if text_len < MIN_TEXT_LEN_FOR_OK:\n",
    "        bad_urls.append(u)\n",
    "\n",
    "if REFETCH_BAD and bad_urls:\n",
    "    bad_urls = sorted(set(bad_urls))\n",
    "    print(f\"Refetching fallback for {len(bad_urls)} URLs via trafilatura.fetch_url ...\")\n",
    "    for u in tqdm(bad_urls, desc=\"Fallback\"):\n",
    "        res = cache.get(u, {})\n",
    "        if res and res.get(\"text_len\", 0) >= MIN_TEXT_LEN_FOR_OK:\n",
    "            continue\n",
    "        try:\n",
    "            html2 = trafilatura.fetch_url(u)\n",
    "            if html2:\n",
    "                t2, tl2, m2, tr2, fl2 = extract_text_strong(html2, u)\n",
    "                images2 = extract_relevant_images(html2, u)\n",
    "                page_title, article_title = res.get(\"page_title\",\"\"), res.get(\"article_title\",\"\")\n",
    "                try:\n",
    "                    soup2 = BeautifulSoup(html2, \"lxml\")\n",
    "                    if soup2.title and soup2.title.string:\n",
    "                        page_title = soup2.title.string.strip() or page_title\n",
    "                    h1 = soup2.find(\"h1\")\n",
    "                    if h1 and h1.get_text(strip=True):\n",
    "                        article_title = h1.get_text(strip=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                ext = tldextract.extract(u)\n",
    "                domain2 = \".\".join([p for p in [ext.domain, ext.suffix] if p])\n",
    "                cache[u] = dict(\n",
    "                    final_url=u if (FORCE_SAME_DOMAIN_FINAL_URL and host_of(u) not in ALLOW_CROSS_DOMAIN_HOSTS) else (res.get(\"final_url\", u) or u),\n",
    "                    http_status=200, error=res.get(\"error\",\"\"),\n",
    "                    domain=domain2, page_title=page_title, article_title=article_title,\n",
    "                    text=t2, text_len=tl2, text_len_full=fl2,\n",
    "                    extraction_method=m2, text_truncated=tr2,\n",
    "                    images_json=json.dumps(images2, ensure_ascii=False), images_count=len(images2)\n",
    "                )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "cols_add = [\"final_url\",\"http_status\",\"error\",\"domain\",\"page_title\",\"article_title\",\"text\",\"text_len\",\"text_len_full\",\"extraction_method\",\"text_truncated\",\"images_json\",\"images_count\"]\n",
    "for c in cols_add:\n",
    "    if c not in valid.columns:\n",
    "        valid[c] = \"\"\n",
    "\n",
    "for idx, row in tqdm(list(valid.iterrows()), total=len(valid), desc=\"Filling\"):\n",
    "    res = cache.get(row[\"norm_url\"])\n",
    "    if res:\n",
    "        for c in cols_add:\n",
    "            valid.at[idx, c] = res[c]\n",
    "\n",
    "for old in [\"top_image\"]:\n",
    "    if old in valid.columns:\n",
    "        valid.drop(columns=[old], inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_out = pd.concat([valid, df[df[\"norm_url\"] == \"\"]], ignore_index=True)\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "unique_ok = sum(1 for u in unique_urls if cache.get(u, {}).get(\"http_status\") == 200 and cache.get(u, {}).get(\"text_len\",0) >= MIN_TEXT_LEN_FOR_OK)\n",
    "print(\"\\n=== Done ===\")\n",
    "print(f\"Unique URLs: {len(unique_urls)}\")\n",
    "print(f\"OK (len(text) >= {MIN_TEXT_LEN_FOR_OK}): {unique_ok}/{len(unique_urls)}\")\n",
    "print(f\"Output: {OUTPUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
